{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting for E-Commerce Sales with Behavioral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pandas numpy matplotlib seaborn scikit-learn tensorflow openpyxl pytrends pmdarima statsmodels scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import skew, kurtosis, probplot\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from pmdarima import auto_arima\n",
    "from pytrends.request import TrendReq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Acquisition and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "retail_data = pd.read_excel(dataset_url)\n",
    "\n",
    "retail_data.dropna(inplace=True)\n",
    "retail_data = retail_data[~retail_data['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "retail_data['Revenue'] = retail_data['Quantity'] * retail_data['UnitPrice']\n",
    "retail_data['InvoiceDate'] = pd.to_datetime(retail_data['InvoiceDate'])\n",
    "\n",
    "sales_by_day = retail_data.set_index('InvoiceDate').resample('D')['Revenue'].sum()\n",
    "complete_date_range = pd.date_range(start=sales_by_day.index.min(), end=sales_by_day.index.max())\n",
    "sales_by_day = sales_by_day.reindex(complete_date_range, fill_value=0)\n",
    "sales_by_day.index.name = 'Date'\n",
    "\n",
    "print(f\"Original dataset dimensions: {retail_data.shape}\")\n",
    "print(f\"Time series length: {sales_by_day.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Exploration of Sales Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sales_by_day.plot(ax=ax, color='steelblue', linewidth=1.5)\n",
    "ax.set_title(\"Revenue Trends Across Time Horizon\", fontsize=15, weight='bold')\n",
    "ax.set_xlabel(\"Timeline\", fontsize=12)\n",
    "ax.set_ylabel(\"Revenue (£)\", fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Assessment and Temporal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test = adfuller(sales_by_day)\n",
    "print(f\"Augmented Dickey-Fuller Statistic: {adf_test[0]:.4f}\")\n",
    "print(f\"Statistical Significance (p): {adf_test[1]:.4f}\")\n",
    "\n",
    "if adf_test[1] > 0.05:\n",
    "    print(\"Series exhibits non-stationarity characteristics\")\n",
    "else:\n",
    "    print(\"Series demonstrates stationarity\")\n",
    "\n",
    "temporal_components = seasonal_decompose(sales_by_day, model='additive', period=30)\n",
    "fig = temporal_components.plot()\n",
    "fig.set_size_inches(14, 8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log1p(sales_by_day)\n",
    "differenced_series = log_transformed.diff().dropna()\n",
    "\n",
    "pre_transform_pval = adfuller(sales_by_day)[1]\n",
    "post_transform_pval = adfuller(differenced_series)[1]\n",
    "print(f\"Pre-transformation p-value: {pre_transform_pval:.4f}\")\n",
    "print(f\"Post-transformation p-value: {post_transform_pval:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "plot_acf(differenced_series, lags=60, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title(\"Autocorrelation Function\", fontsize=13, weight='bold')\n",
    "axes[0].set_xlabel(\"Lag Period\")\n",
    "axes[0].set_ylabel(\"Correlation Coefficient\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "plot_pacf(differenced_series, lags=60, ax=axes[1], method='ywm', alpha=0.05)\n",
    "axes[1].set_title(\"Partial Autocorrelation Function\", fontsize=13, weight='bold')\n",
    "axes[1].set_xlabel(\"Lag Period\")\n",
    "axes[1].set_ylabel(\"Partial Correlation\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Identification and Data Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 30\n",
    "percentile_25 = sales_by_day.rolling(rolling_window, center=True).quantile(0.25)\n",
    "percentile_75 = sales_by_day.rolling(rolling_window, center=True).quantile(0.75)\n",
    "interquartile_range = percentile_75 - percentile_25\n",
    "\n",
    "threshold_lower = percentile_25 - 1.5 * interquartile_range\n",
    "threshold_upper = percentile_75 + 1.5 * interquartile_range\n",
    "iqr_anomalies = (sales_by_day < threshold_lower) | (sales_by_day > threshold_upper)\n",
    "\n",
    "standardized_scores = (sales_by_day - sales_by_day.mean()) / sales_by_day.std()\n",
    "zscore_anomalies = np.abs(standardized_scores) > 3\n",
    "\n",
    "combined_anomalies = iqr_anomalies | zscore_anomalies\n",
    "\n",
    "sanitized_series = sales_by_day.copy()\n",
    "median_imputation = sales_by_day.rolling(window=7, center=True).median()\n",
    "sanitized_series[combined_anomalies] = median_imputation[combined_anomalies]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(sales_by_day, label='Raw Data', alpha=0.5, color='lightgray', linewidth=1)\n",
    "ax.plot(sanitized_series, label='Sanitized Data', linewidth=2, color='darkblue')\n",
    "ax.scatter(sales_by_day[combined_anomalies].index, sales_by_day[combined_anomalies].values,\n",
    "           color='crimson', s=60, label='Detected Anomalies', zorder=5)\n",
    "ax.set_title(\"Hybrid Anomaly Detection with Median Imputation\", fontsize=14, weight='bold')\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Revenue\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "split_index = int(len(sales_by_day) * split_ratio)\n",
    "training_set, validation_set = sales_by_day[:split_index], sales_by_day[split_index:]\n",
    "\n",
    "print(f\"Training observations: {len(training_set)}\")\n",
    "print(f\"Validation observations: {len(validation_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holt-Winters Exponential Smoothing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_model = ExponentialSmoothing(training_set, seasonal='add', seasonal_periods=30)\n",
    "hw_fitted = hw_model.fit()\n",
    "hw_predictions = hw_fitted.forecast(len(validation_set))\n",
    "\n",
    "hw_mae = mean_absolute_error(validation_set, hw_predictions)\n",
    "hw_rmse = np.sqrt(mean_squared_error(validation_set, hw_predictions))\n",
    "hw_r2 = r2_score(validation_set, hw_predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error: {hw_mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {hw_rmse:.2f}\")\n",
    "print(f\"Coefficient of Determination: {hw_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated ARIMA Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = auto_arima(\n",
    "    training_set,\n",
    "    start_p=1, start_q=1,\n",
    "    max_p=5, max_q=5,\n",
    "    d=None,\n",
    "    seasonal=False,\n",
    "    stepwise=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore',\n",
    "    trace=True\n",
    ")\n",
    "\n",
    "optimal_parameters = auto_model.order\n",
    "print(f\"\\nOptimal ARIMA configuration: {optimal_parameters}\")\n",
    "\n",
    "arima_model = ARIMA(training_set, order=optimal_parameters)\n",
    "arima_fitted = arima_model.fit()\n",
    "arima_predictions = arima_fitted.forecast(steps=len(validation_set))\n",
    "\n",
    "print(f\"Generated {len(arima_predictions)} forecast points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(actual, predicted):\n",
    "    valid_mask = actual != 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((actual[valid_mask] - predicted[valid_mask]) / actual[valid_mask])) * 100\n",
    "\n",
    "def compute_metrics(actual, predicted, name='Model'):\n",
    "    mae_score = mean_absolute_error(actual, predicted)\n",
    "    rmse_score = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mape_score = calculate_mape(actual, predicted)\n",
    "    r2_score_val = r2_score(actual, predicted)\n",
    "    bias_score = np.mean(predicted - actual)\n",
    "    \n",
    "    return {\n",
    "        'Algorithm': name,\n",
    "        'MAE': round(mae_score, 2),\n",
    "        'RMSE': round(rmse_score, 2),\n",
    "        'MAPE (%)': round(mape_score, 2),\n",
    "        'R²': round(r2_score_val, 4),\n",
    "        'Bias': round(bias_score, 2)\n",
    "    }\n",
    "\n",
    "performance_metrics = []\n",
    "performance_metrics.append(compute_metrics(validation_set, hw_predictions, 'Holt-Winters'))\n",
    "performance_metrics.append(compute_metrics(validation_set, arima_predictions, 'Auto-ARIMA'))\n",
    "\n",
    "comparison_table = pd.DataFrame(performance_metrics)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(comparison_table.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "ax.plot(validation_set.index, validation_set, label='Observed Values', \n",
    "        color='black', linewidth=2.5, marker='o', markersize=4)\n",
    "ax.plot(validation_set.index, hw_predictions, label='Holt-Winters Forecast', \n",
    "        linestyle='--', marker='s', markersize=5, color='darkorange', alpha=0.8)\n",
    "ax.plot(validation_set.index, arima_predictions, label='ARIMA Forecast', \n",
    "        linestyle='-.', marker='^', markersize=5, color='darkgreen', alpha=0.8)\n",
    "\n",
    "ax.set_title('Predictive Model Performance Comparison', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Time Period', fontsize=13)\n",
    "ax.set_ylabel('Revenue (£)', fontsize=13)\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "ax.grid(alpha=0.4, linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Error Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_errors = validation_set - hw_predictions\n",
    "error_mean = forecast_errors.mean()\n",
    "error_std = forecast_errors.std()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(forecast_errors, color='darkred', linewidth=1.8)\n",
    "axes[0].axhline(0, color='black', linestyle='--', linewidth=1.2)\n",
    "axes[0].fill_between(forecast_errors.index, error_mean - 2*error_std, \n",
    "                     error_mean + 2*error_std, alpha=0.25, color='gray')\n",
    "axes[0].set_title('Temporal Error Distribution', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residual Value')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(forecast_errors, bins=25, edgecolor='black', alpha=0.75, color='salmon')\n",
    "axes[1].axvline(0, color='darkred', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Error Frequency Distribution', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Residual Magnitude')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "probplot(forecast_errors, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Normality Assessment (Q-Q Plot)', fontsize=12, weight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ljung_box_test = acorr_ljungbox(forecast_errors, lags=[10], return_df=True)\n",
    "print(\"\\nLjung-Box Autocorrelation Test:\")\n",
    "print(ljung_box_test)\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"Mean: {error_mean:.2f}\")\n",
    "print(f\"Standard Deviation: {error_std:.2f}\")\n",
    "print(f\"Skewness: {skew(forecast_errors):.2f}\")\n",
    "print(f\"Kurtosis: {kurtosis(forecast_errors):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Trend Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_api = TrendReq(hl='en-US', tz=360)\n",
    "search_terms = ['online shopping', 'e-commerce']\n",
    "\n",
    "period_start = sales_by_day.index.min().strftime('%Y-%m-%d')\n",
    "period_end = sales_by_day.index.max().strftime('%Y-%m-%d')\n",
    "query_timeframe = f'{period_start} {period_end}'\n",
    "\n",
    "trends_api.build_payload(kw_list=search_terms, cat=0, timeframe=query_timeframe, geo='GB', gprop='')\n",
    "trends_data = trends_api.interest_over_time()\n",
    "\n",
    "if 'isPartial' in trends_data.columns:\n",
    "    trends_data = trends_data.drop(columns=['isPartial'])\n",
    "\n",
    "trends_data.columns = [f\"{col.replace(' ', '_').lower()}_index\" for col in trends_data.columns]\n",
    "trends_daily = trends_data.resample('D').ffill()\n",
    "\n",
    "sales_by_day.index.name = 'Date'\n",
    "trends_daily.index.name = 'Date'\n",
    "\n",
    "enriched_dataset = pd.merge(\n",
    "    sales_by_day.to_frame(name='Revenue'),\n",
    "    trends_daily,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "enriched_dataset.fillna(method='ffill', inplace=True)\n",
    "enriched_dataset.fillna(method='bfill', inplace=True)\n",
    "enriched_dataset.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Enhanced dataset dimensions: {enriched_dataset.shape}\")\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(enriched_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Pattern Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineered = enriched_dataset.copy()\n",
    "\n",
    "feature_engineered['weekday'] = feature_engineered.index.dayofweek\n",
    "feature_engineered['month_num'] = feature_engineered.index.month\n",
    "feature_engineered['quarter_num'] = feature_engineered.index.quarter\n",
    "feature_engineered['weekend_flag'] = feature_engineered.index.dayofweek.isin([5, 6]).astype(int)\n",
    "feature_engineered['iso_week'] = feature_engineered.index.isocalendar().week.astype(int)\n",
    "\n",
    "feature_engineered['ma_7day'] = feature_engineered['Revenue'].rolling(window=7).mean()\n",
    "feature_engineered['volatility_7day'] = feature_engineered['Revenue'].rolling(window=7).std()\n",
    "\n",
    "feature_engineered['revenue_lag1'] = feature_engineered['Revenue'].shift(1)\n",
    "feature_engineered['revenue_lag7'] = feature_engineered['Revenue'].shift(7)\n",
    "feature_engineered['revenue_lag30'] = feature_engineered['Revenue'].shift(30)\n",
    "\n",
    "imputation_cols = ['ma_7day', 'volatility_7day', 'revenue_lag1', 'revenue_lag7', 'revenue_lag30']\n",
    "for col in imputation_cols:\n",
    "    feature_engineered[col].fillna(feature_engineered[col].mean(), inplace=True)\n",
    "\n",
    "clustering_features = [\n",
    "    'online_shopping_index', 'e-commerce_index',\n",
    "    'weekday', 'month_num', 'quarter_num', 'weekend_flag', 'iso_week',\n",
    "    'ma_7day', 'volatility_7day',\n",
    "    'revenue_lag1', 'revenue_lag7', 'revenue_lag30'\n",
    "]\n",
    "\n",
    "feature_matrix = feature_engineered[clustering_features]\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "normalized_features = normalizer.fit_transform(feature_matrix)\n",
    "\n",
    "cluster_range = range(2, 12)\n",
    "inertia_values = []\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    clustering_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clustering_model.fit(normalized_features)\n",
    "    inertia_values.append(clustering_model.inertia_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "ax.plot(cluster_range, inertia_values, marker='D', linestyle='-', linewidth=2, markersize=8, color='darkviolet')\n",
    "ax.set_title('Optimal Cluster Determination via Elbow Method', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('Number of Clusters', fontsize=12)\n",
    "ax.set_ylabel('Within-Cluster Sum of Squares', fontsize=12)\n",
    "ax.set_xticks(cluster_range)\n",
    "ax.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters = 4\n",
    "final_clustering = KMeans(n_clusters=selected_clusters, random_state=42, n_init=10)\n",
    "feature_engineered['segment'] = final_clustering.fit_predict(normalized_features)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "color_palette = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "for segment_id in range(selected_clusters):\n",
    "    segment_subset = feature_engineered[feature_engineered['segment'] == segment_id]\n",
    "    ax.scatter(segment_subset.index, segment_subset['Revenue'], \n",
    "              label=f'Segment {segment_id}', alpha=0.7, s=50, color=color_palette[segment_id])\n",
    "\n",
    "ax.set_title('Consumer Behavior Segmentation Analysis', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Timeline', fontsize=13)\n",
    "ax.set_ylabel('Revenue (£)', fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "ax.grid(alpha=0.3, linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSegment-wise Revenue Statistics:\")\n",
    "print(\"=\"*80)\n",
    "segment_summary = feature_engineered.groupby('segment')['Revenue'].describe()\n",
    "print(segment_summary)\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
